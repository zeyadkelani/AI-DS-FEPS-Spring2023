{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a994a7fe",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Algorithms \n",
    "\n",
    "Supervised learning is the types of machine learning in which machines are trained using well \"labelled\" training data, and on basis of that data, machines predict the output. The labelled data means some input data is already tagged with the correct output.\n",
    "\n",
    "Supervised learning is a process of providing input data as well as correct output data to the machine learning model. The aim of a supervised learning algorithm is to find a mapping function to map the input variable(x) with the output variable(y).\n",
    "\n",
    "In the real-world, supervised learning can be used for Risk Assessment, Image classification, Fraud Detection, spam filtering, etc.\n",
    "\n",
    "In this section, we will cover:\n",
    "* K-Nearest Neighbors \n",
    "* Decision/Regression Trees\n",
    "* Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddda4e8",
   "metadata": {},
   "source": [
    "## K- Nearest Neighbors (KNN) ML Algorithm \n",
    "\n",
    "* K-Nearest Neighbour is one of the simplest Supervised Machine Learning algorithms.\n",
    "* K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
    "* K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "* K-NN algorithm can be used for Regression [to predict numeric target variable] as well as for Classification [to predict categorical target variable] but **mostly it is used for the Classification problems.**\n",
    "* K-NN is a **non-parametric algorithm**, which means it does not make any assumption on underlying data.\n",
    "* The K-Nearest Neighbor algorithm works by calculating a new data points class (in the case of classification) or value (in the case of regression) by looking at its most similar neighbors. \n",
    "    * How does it determine which data points are the most similar? Generally, this is done by using a distance calculation, such as the Euclidian distance or the Manhattan distance.\n",
    "        * Euclidean distance is a widely used distance metric. It works on the principle of the Pythagoras theorem and signifies the shortest distance between two points.\n",
    "        * Manhattan distance measures the distance that a taxi-cab would have to take if it could only make right-angle turns.\n",
    "        * Also, the Minkowski distance or Minkowski metric is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance. \n",
    "        \n",
    "Official documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def029a",
   "metadata": {},
   "source": [
    "We will take an example of using the KNN algorithm for classification purposes.\n",
    "\n",
    "In this section, we will use a built-in dataset in the seaborn package (https://seaborn.pydata.org/generated/seaborn.load_dataset.html): the dataset focuses on predicting the species of a penguin based on its physical characteristics. Seaborn is another great Python package that focuses on data exploration and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcb0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from seaborn import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4f954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "df = load_dataset('penguins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56e48fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    Male  \n",
       "1       3800.0  Female  \n",
       "2       3250.0  Female  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  Female  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcce9895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            344 non-null    object \n",
      " 1   island             344 non-null    object \n",
      " 2   bill_length_mm     342 non-null    float64\n",
      " 3   bill_depth_mm      342 non-null    float64\n",
      " 4   flipper_length_mm  342 non-null    float64\n",
      " 5   body_mass_g        342 non-null    float64\n",
      " 6   sex                333 non-null    object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 18.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42beea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species                3\n",
       "island                 3\n",
       "bill_length_mm       164\n",
       "bill_depth_mm         80\n",
       "flipper_length_mm     55\n",
       "body_mass_g           94\n",
       "sex                    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d02b35",
   "metadata": {},
   "source": [
    "#### Some information about the variables available in this dataset:\n",
    "* species: The species of the penguin\n",
    "* island: The island on which the penguin's data was taken\n",
    "* bill_length_mm:The length of the penguin’s bill, measured in millimetres\n",
    "* bill_depth_mm: The depth of the penguin’s bill, measured in millimetres\n",
    "* flipper_length_mm: The length of the penguin’s flipper, measured in millimetres\n",
    "* body_mass_g: The mass of the penguin, measured in grams\n",
    "* sex: The sex of the penguin\n",
    "\n",
    "#### To practice:\n",
    "1. We will set up KNN algorithm with a target value and one feature\n",
    "2. Then set up KNN algorithm with a target value and all numerical features \n",
    "3. Set up KNN algorithm with a target value and all features (including categorical features) \n",
    "\n",
    "And for each of the above, we will look at the accuracy of the model and how to evaluate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b5716",
   "metadata": {},
   "source": [
    "#### 1. Using just one feature:\n",
    "For example, we will only classify the species of the penguin based on the bill's length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee35361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data: dropping NA values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b44596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 333 entries, 0 to 343\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            333 non-null    object \n",
      " 1   island             333 non-null    object \n",
      " 2   bill_length_mm     333 non-null    float64\n",
      " 3   bill_depth_mm      333 non-null    float64\n",
      " 4   flipper_length_mm  333 non-null    float64\n",
      " 5   body_mass_g        333 non-null    float64\n",
      " 6   sex                333 non-null    object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 20.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66f694bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining our X (features array: usually multi-dimensional array)\n",
    "# an y (target array: expected to be of a single dimension)\n",
    "\n",
    "X = df[['bill_length_mm']] # Usually more than one, which is why we are giving it a list of the variables. In this case, it is just one.\n",
    "y = df['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef0fb3",
   "metadata": {},
   "source": [
    "We will now split our data into a training dataset and a testing dataset. \n",
    "\n",
    "This is a very important step because a poorly split dataset, or one that’s not split at all, can lead to two common problems in machine learning. Namely, these problems are referred to as **underfitting** and **overfitting** a model.\n",
    "\n",
    "**Underfitting is a problem that occurs when a model doesn’t capture the relationships between different variables.**\n",
    "\n",
    "A common cause of this can be when, say, including the incorrect variables. Similarly, it can occur when the wrong type of model is applied to a given problem. For example, applying a polynomial model to a model that actually linear. This type of problem will perform poorly in both training and testing data. Because of this, it can be easy to spot.\n",
    "\n",
    "**On the other hand, overfitting occurs when the model attempts to find overly complex relationships between variables that don’t actually exist.**\n",
    "\n",
    "This is typically a problem when the dataset learns from both the true relationships (the “signal”) and from variables that have little influence (the “noise”). Generally, these types of models perform exceptionally well with training data, but quite poorly with testing data.\n",
    "\n",
    "**To split the data, we will use sklearn's train_test_split function.** The only required parameter is the **arrays** of the features' and targets' arrays. \n",
    "\n",
    "You can also decide things, such as:\n",
    "* **test_size**: defaults to None and is 0.25 of the data size if train_size is also None\n",
    "* **train_size**: defaults to None, complement of test_size\n",
    "* **random_state**: defaults to None, and it is a parameter that controls shuffling applied to the data before applying the split. It is good practice to pass it an integer for reproducibility reasons: similar to when we set the random seed\n",
    "* **shuffle**: defaults to True; whether or not to shuffle the data before splitting > if it is set to False, stratify must be None\n",
    "* **stratify**: defaults to None, if not None, the data is split in a stratified fashion > especially helpful when one is trying to classify an imbalanced dataset, where there isn’t a balance between the different classes.\n",
    "\n",
    "A simple explanation of what stratifying is\n",
    "\"stratifying preserves the proportion of how data is distributed in the target column - and depicts that same proportion of distribution in the train_test_split. Take for example, if the problem is a binary classification problem, and the target column is having proportion of 80% = yes, and 20% = no. Since there are 4 times more 'yes' than 'no' in the target column, by splitting into train and test without stratifying, we might run into the trouble of having only the 'yes' falling into our training set, and all the 'no' falling into our test set.(i.e, the training set might not have 'no' in its target column)\n",
    "\n",
    "Hence by Stratifying, the target column for the training set has 80% of 'yes' and 20% of 'no', and also, the target column for the test set has 80% of 'yes' and 20% of 'no' respectively.\n",
    "\n",
    "Hence, Stratify makes even distribution of the target(label) in the train and test set - just as it is distributed in the original dataset.\" [https://stackoverflow.com/a/72092663]\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76dcc23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adelie       146\n",
       "Gentoo       119\n",
       "Chinstrap     68\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['species'].value_counts() #Relatively imbalanced data, so will use stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3728329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into a training dataset and a testing dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 100, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e9d3c",
   "metadata": {},
   "source": [
    "Before using the KNeighborsClassifier function, let's understand its parameters. Remember you can always check the official documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "Parameters:\n",
    "* n_neighbors: default is 5, determine the number of neighbors that the algorithm uses to classify a certain data point. It is usually recommended to use odd number of neighbors.\n",
    "* weights: default is uniform; there is also the option to weight by distance, where weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "* algorithm: algorithm used to compute the nearest neighbors; the default is auto, which will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "* leaf_size: default to 30, specific to certain algorithms of the above.\n",
    "* p: default to 2. It is the power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. \n",
    "* metric: default to minkowski; Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2. \n",
    "* metric_paramsdict, default to None; Additional keyword arguments for the metric function.\n",
    "* n_jobs: default is None, which is 1. It is the number of parallel jobs to run for neighbors search. -1 is to use all processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599c4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set up the classifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fd2a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(weights='distance')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f16199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now onto predicting\n",
    "\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95ab501e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adelie' 'Adelie' 'Gentoo' 'Adelie' 'Adelie' 'Gentoo' 'Adelie' 'Adelie'\n",
      " 'Chinstrap' 'Gentoo' 'Adelie' 'Chinstrap' 'Adelie' 'Gentoo' 'Gentoo'\n",
      " 'Chinstrap' 'Gentoo' 'Gentoo' 'Adelie' 'Chinstrap' 'Chinstrap'\n",
      " 'Chinstrap' 'Gentoo' 'Chinstrap' 'Gentoo' 'Chinstrap' 'Gentoo' 'Gentoo'\n",
      " 'Adelie' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Gentoo' 'Chinstrap'\n",
      " 'Chinstrap' 'Gentoo' 'Gentoo' 'Adelie' 'Adelie' 'Gentoo' 'Chinstrap'\n",
      " 'Adelie' 'Gentoo' 'Adelie' 'Adelie' 'Gentoo' 'Adelie' 'Adelie' 'Adelie'\n",
      " 'Chinstrap' 'Adelie' 'Gentoo' 'Adelie' 'Adelie' 'Chinstrap' 'Adelie'\n",
      " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Gentoo' 'Adelie' 'Chinstrap'\n",
      " 'Adelie' 'Adelie' 'Gentoo' 'Adelie' 'Gentoo' 'Chinstrap' 'Chinstrap'\n",
      " 'Gentoo' 'Adelie' 'Adelie' 'Adelie' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
      " 'Adelie' 'Gentoo' 'Chinstrap' 'Adelie' 'Adelie' 'Gentoo' 'Adelie']\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "069e0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gentoo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noura\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predicting on something not from the dataset\n",
    "\n",
    "predictions = clf.predict([[44.2]]) # x: a penguin's bill length\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f47fbd",
   "metadata": {},
   "source": [
    "### Validating our KNN algorithm\n",
    "Since our original dataset is pre-labelled, we can assess how accurate our model is. \n",
    "\n",
    "Because we split our data into training and testing data, it can be helpful to evaluate the model’s performance using the testing data. This is because this is data that the model hasn’t yet seen. Because of this, we can be confident that the model’s effectiveness to new data can be accurately tested.\n",
    "\n",
    "In classification problems, one helpful measurement for a model’s effectiveness is the accuracy score. This looks at the proportion of accurate predictions out of the total of all predictions.\n",
    "\n",
    "When we made predictions using the X_test array, sklearn returned an array of predictions. We already know the true values for these: they’re stored in y_test.\n",
    "\n",
    "We can use the sklearn function, accuracy_score() to return a proportion out of 1 that measures the algorithms effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03e34aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161f878",
   "metadata": {},
   "source": [
    "Not exactly bad, but we can do better by adding more of the available features.\n",
    "\n",
    "#### 2. Using all numeric features:\n",
    "For example, we will classify the penguins' species based on all numeric features, such as the bill's length & depth, flipper length, and body mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b42cab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data, just to give ourselves a clean slate\n",
    "df = load_dataset('penguins')\n",
    "\n",
    "# Cleaning the data\n",
    "df = df.dropna()\n",
    "\n",
    "# Creating our X and y\n",
    "X = df.select_dtypes(include='number')\n",
    "y = df['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd831e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n",
       "0            39.1           18.7              181.0       3750.0\n",
       "1            39.5           17.4              186.0       3800.0\n",
       "2            40.3           18.0              195.0       3250.0\n",
       "4            36.7           19.3              193.0       3450.0\n",
       "5            39.3           20.6              190.0       3650.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75e2b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into a training dataset and a testing dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 100, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1bdc882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the classifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "548cc1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(weights='distance')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a0f074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our model/making predictions\n",
    "\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e2c9a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy score \n",
    "\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea27a5d",
   "metadata": {},
   "source": [
    "Such a great improvement! Let's also try to add our categorical variables, such as sex and island. \n",
    "\n",
    "#### 3. Using all available features:\n",
    "Machine learning models work with numerical data. Because of this, we need to transform the data in our categorical columns into numbers in order for our algorithm to work successfully.\n",
    "\n",
    "There are a number of different ways in which we can encode our categorical data. One of these methods is known as **one-hot encoding.** This process converts each unique value in a categorical column into its own binary column. So, for example, after applying it to the sex column, we will have 2 new columns: female and male. After applying it to the island, we will have 3 new columns, each for an island. In this case, the outcome is binary for each column: 0 means the value is not presented, while 1 means the value is presented.\n",
    "\n",
    "We will not encode them into 0,1,2 because it entails that there is an order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f137ca",
   "metadata": {},
   "source": [
    "### Preprocessing before applying our third algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ab924cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot encoding our categorical variables \n",
    "\n",
    "# Creating our updated/comprehensive X\n",
    "\n",
    "X = df.drop(columns = ['species'])\n",
    "\n",
    "# Resplitting our data based on the current X and y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 100, stratify = y)\n",
    "\n",
    "#Using the make_column_transformer to apply the OneHotEncode() function\n",
    "\n",
    "# Documentation> https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \n",
    "\n",
    "column_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['sex', 'island']), # apply it on these columns\n",
    "    remainder='passthrough') # ignore all other columns\n",
    "\n",
    "# using the fit_transform() function to apply the column_transformer on the X_train data\n",
    "\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train, columns=column_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42c096",
   "metadata": {},
   "source": [
    "Since our data is made up of different variables, and some have larger ranges than others. Ex: transformed categorical variables have 0 and 1, while body mass has a range 2700-6300. It is always a good idea to scale the data to make sure the larger range does not \"dominate\" the algorithm. \n",
    "\n",
    "Suggested in this case to use a Min-Max normalization method, which will turn all values to be a range from 0 to 1. The original distribution will be maintained.\n",
    "\n",
    "To apply the Min-Max scaling, we will do very similar steps to the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b2b6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading and recleaning the data; just to ensure nothing is out of place\n",
    "df = load_dataset('penguins')\n",
    "df = df.dropna()\n",
    "\n",
    "# Recreating our comprehensive X and y\n",
    "X = df.drop(columns = ['species'])\n",
    "y = df['species']\n",
    "\n",
    "# Splitting the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 100, stratify = y)\n",
    "\n",
    "# Using make_column_transformer() to apply OneHotEncoder() and MinMaxScaler()\n",
    "# Documentation > https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "column_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['sex', 'island']),\n",
    "    (MinMaxScaler(), ['bill_depth_mm', 'bill_length_mm', 'flipper_length_mm', 'body_mass_g']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "# using the fit_transform() function to apply the column_transformer on the X_train data\n",
    "\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train, columns=column_transformer.get_feature_names_out())\n",
    "\n",
    "# Apply the same on the testing data, but using the transform () function instead of fit_transform()\n",
    "\n",
    "X_test = column_transformer.transform(X_test)\n",
    "X_test = pd.DataFrame(data=X_test, columns=column_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ec751",
   "metadata": {},
   "source": [
    "### Applying our third algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51f89397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our classifier\n",
    "clf = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46dabe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(weights='distance')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training/fitting it\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f66aac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing/predicting \n",
    "\n",
    "predictons = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7870a6b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy of the third algorithm\n",
    "\n",
    "print(accuracy_score(y_test, predictions)) # Show them if I change the sample stratification above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc66a1",
   "metadata": {},
   "source": [
    "### More advanced learning opportunities:\n",
    "\n",
    "1. Other methods of validating and evaluating the algorithm:\n",
    "\n",
    "Such as a confusion matrix, also known as an error matrix, is a powerful tool used to evaluate the performance of classification models. \n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "Helpful tutorial: https://www.jcchouinard.com/confusion-matrix-in-scikit-learn/ \n",
    "\n",
    "2. Hyper-parameter tuning:\n",
    "\n",
    "Hyper-parameters are the variables that you specify while building a machine learning model. This includes, for example, the number of neighbours to consider or the type of distance to use.\n",
    "\n",
    "Hyper-parameter tuning, then, refers to the process of tuning these values to ensure a higher accuracy score. One way to do this is, simply, to plug in different values and see which hyper-parameters return the highest score.\n",
    "\n",
    "This, however, is quite time-consuming. Scikit-Learn comes with a function **GridSearchCV** which makes the process simpler. You simply provide a dictionary of values to run through and sklearn returns the values that worked best.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "This function completes **a process of cross-validation.** This means that it will cycle through different combinations of training and testing data, in order to help prevent overfitting. For example, when we set a test size of 20%, cross-validation will cycle through different splits of that 20% in relation to the whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623bdd7c",
   "metadata": {},
   "source": [
    "## Decision Trees ML Algorithm\n",
    "* Decision tree classifiers are supervised machine learning models. \n",
    "* Decision trees **can also be used for regression problems.** \n",
    "* Decision tree classifiers work like flowcharts. \n",
    "    * Each node of a decision tree represents a decision point that splits into two leaf nodes. \n",
    "    * Each of these nodes represents the outcome of the decision and each of the decisions can also turn into decision nodes. \n",
    "    * Eventually, the different decisions will lead to a final classification.\n",
    "* Decision trees work to make decisions (show flowchart example). The top node is called the root node. Each of the decision points are called decision nodes. The final decision point is referred to as a leaf node.\n",
    "* Decision trees are easy to understand and interpret its decision-making algorithm. \n",
    "* Ther are generally faster to train than other algorithms, such as neural networks.\n",
    "* Their complexity is a by-product of the data’s attributes and dimensions.\n",
    "* They can handle high dimensional data with high degrees of accuracy.\n",
    "* It’s a **non-parametric method** meaning that they do not depend on probability distribution assumptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be9030",
   "metadata": {},
   "source": [
    "This following part is alreadty automated using the scikit-learn package; however, it is useful to understand the background.\n",
    "\n",
    "The algorithm uses a number of different ways to split the dataset into a series of decisions. One of the ways is **Gini Impurity.**\n",
    "\n",
    "Useful resources:\n",
    "- https://stats.stackexchange.com/a/69048 [Explains Gini Gain and its calculations simply]\n",
    "- https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c\n",
    "\n",
    "Gini Impurity refers to a measurement of the likelihood of incorrect classification of a new instance of a random variable *if that instance was randomly classified according to the distribution of class labels from the dataset.*\n",
    "\n",
    "**The Gini Impurity measures the likelihood that an item will be misclassified if it’s randomly assigned a class based on the data’s distribution.**\n",
    "\n",
    "When training a decision tree, the best split is chosen by maximizing the Gini Gain, which is calculated by subtracting the weighted impurities of the branches from the original impurity.\n",
    "\n",
    "In this exercise, we will use the titanic dataset. \n",
    "\n",
    "To practice, we will:\n",
    "1. Apply the Decision Tree to all numeric data\n",
    "2. Apply the Decision Tree to all available features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6dfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('TitanicFull.csv') # make sure to use your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffc62eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Summary of the dataset\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d6933a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    891\n",
       "Survived         2\n",
       "Pclass           3\n",
       "Name           891\n",
       "Sex              2\n",
       "Age             88\n",
       "SibSp            7\n",
       "Parch            7\n",
       "Ticket         681\n",
       "Fare           248\n",
       "Cabin          147\n",
       "Embarked         3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d7148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns and mainly null values\n",
    "\n",
    "df.drop(['Name', 'Ticket', 'Cabin'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbf0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing NA values\n",
    "\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b55f28cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e8919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 2 missing values for Embarked \n",
    "df.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bd4c79f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c226f",
   "metadata": {},
   "source": [
    "### 1. Apply the Decision Tree to all numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa974b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our X and y\n",
    "X = df.select_dtypes(include='number')\n",
    "\n",
    "# Survived is an integer data type as well, but it cannot be in both our X and y\n",
    "X.drop(['Survived', 'PassengerId', 'Fare'], axis = 1, inplace=True)\n",
    "# Also dropped PassengerId and Fare.\n",
    "\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9b54f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass        Age  SibSp  Parch\n",
       "0         3  22.000000      1      0\n",
       "1         1  38.000000      1      0\n",
       "2         3  26.000000      0      0\n",
       "3         1  35.000000      1      0\n",
       "4         3  35.000000      0      0\n",
       "..      ...        ...    ...    ...\n",
       "886       2  27.000000      0      0\n",
       "887       1  19.000000      0      0\n",
       "888       3  29.699118      1      2\n",
       "889       1  26.000000      0      0\n",
       "890       3  32.000000      0      0\n",
       "\n",
       "[889 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a45cd870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    340\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the data is a bit imbalanced, we will use stratify when splitting data \n",
    "df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cfdf1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 100, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ed702",
   "metadata": {},
   "source": [
    "Before using the DecisionTreeClassifier function, let's understand its parameters. Remember you can always check the official documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "Parameters:\n",
    "* criterion: its default is\t'gini'. The function to measure the quality of a split. There is also 'entropy' or 'log_loss'\n",
    "* splitter: its default is 'best'. The strategy to choose the best split. Either 'best' or 'random'\n",
    "* max_depth: its default is\tNone. The maximum depth of the tree. If None, the nodes are expanded until all leaves are pure or until they contain less than the min_samples_split\n",
    "* min_samples_split: its default is\t2. The minimum number of samples required to split a node.\n",
    "* min_samples_leaf: its default is 1. The minimum number of samples require to be at a leaf node.\n",
    "* min_weight_fraction_leaf=\t0.0\tThe minimum weighted fraction of the sum of weights of all the input samples required to be at a node.\n",
    "* max_features: the default is None. The number of features to consider when looking for the best split. Can be:\n",
    "\n",
    "    – int,\n",
    "    \n",
    "    – float,\n",
    "    \n",
    "    – 'auto' (the square root of number of features),\n",
    "    \n",
    "    – 'sqrt' (same as auto),\n",
    "    \n",
    "    – 'log2' (log of number of features),\n",
    "    \n",
    "    – None (the number of features)\n",
    "    \n",
    "* random_state: its default is None. The control for the randomness of the estimator.\n",
    "* max_leaf_nodes: its default is None. Grow a tree with a maximum number of nodes. If None, then an unlimited number is possible.\n",
    "* min_impurity_decrease: its default is 0.0. A node will be split if this split decreases the impurity greater than or equal to this value.\n",
    "\n",
    "\n",
    "**In this section, we will focus on criterion, max_depth, max_features, and splitter.** Since we are beginners, we can depend on the fact that the scikit-learn automizes a lot of the complex decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b5d4ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our classifier \n",
    "\n",
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf92ca94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the algorithm \n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11619550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing/predicting the algorithm\n",
    "\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2b66815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d2c11cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6591928251121076\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy of our model \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc95ec0",
   "metadata": {},
   "source": [
    "### 2. Apply the Decision Tree to all features\n",
    "\n",
    "Remember we have to first transform the categorical variables using OneHotEncoder()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f175cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our X and y\n",
    "X = df.copy()\n",
    "\n",
    "# Survived is part of the dataset, but it cannot be in both our X and y\n",
    "X.drop(['Survived', 'PassengerId', 'Fare'], axis = 1, inplace=True)\n",
    "\n",
    "\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c82253d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex   Age  SibSp  Parch Embarked\n",
       "0       3    male  22.0      1      0        S\n",
       "1       1  female  38.0      1      0        C\n",
       "2       3  female  26.0      0      0        S\n",
       "3       1  female  35.0      1      0        S\n",
       "4       3    male  35.0      0      0        S"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08aa433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 100, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc0bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming our columns for the train data:\n",
    "\n",
    "column_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['Sex', 'Embarked']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train, columns=column_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "905adeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming our columns for the test data:\n",
    "\n",
    "X_test = column_transformer.transform(X_test)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test, columns=column_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88fee3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our classifier \n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Training the algorithm \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Testing/predicting the algorithm\n",
    "\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f6a6fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0829078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7892376681614349\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy of our model \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a43098",
   "metadata": {},
   "source": [
    "Although many machine learning algorithms are based on distance calculations, it is not the same for decision trees. Thus, we do not need to worry about scaling or normalizing data when using decision tree algorithms.\n",
    "\n",
    "\n",
    "### More advanced learning opportunities:\n",
    "\n",
    "1. Other methods of validating and evaluating the algorithm:\n",
    "\n",
    "Such as a confusion matrix, also known as an error matrix, is a powerful tool used to evaluate the performance of classification models. \n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "Helpful tutorial: https://www.jcchouinard.com/confusion-matrix-in-scikit-learn/ \n",
    "\n",
    "2. Hyper-parameter tuning:\n",
    "\n",
    "Hyper-parameters are the variables that you specify while building a machine learning model. This includes, for example, how the algorithm splits the data (either by entropy or gini impurity), as well as, the other parameters mentioned above.\n",
    "\n",
    "Hyper-parameter tuning, then, refers to the process of tuning these values to ensure a higher accuracy score. One way to do this is, simply, to plug in different values and see which hyper-parameters return the highest score.\n",
    "\n",
    "This, however, is quite time-consuming. Scikit-Learn comes with a function **GridSearchCV** which makes the process simpler. You simply provide a dictionary of values to run through and sklearn returns the values that worked best.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "This function completes **a process of cross-validation.** This means that it will cycle through different combinations of training and testing data, in order to help prevent overfitting. For example, when we set a test size of 20%, cross-validation will cycle through different splits of that 20% in relation to the whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896807f2",
   "metadata": {},
   "source": [
    "## Random Forests ML Algorithm \n",
    "* A random forest classifier is an ensemble algorithm. \n",
    "* An ensemble algorithm leverages multiple instances of another algorithm at the same time to find a result. \n",
    "* Decision trees are prone to overfitting. \n",
    "    * By \"planting\" more decision trees, one can avoid the overfitting that happens with decision trees.\n",
    "    * A random forest is the automated handling of creating more decision trees. Each tree receives a vote in terms of how to classify. Some of these votes will be wildly overfitted and inaccurate. However, by creating a hundred trees the classification returned by the most trees is very likely to be the most accurate.\n",
    "    \n",
    "For this example, we will be using the Penguins dataset again but try to apply it to the titanic dataset too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf6a3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "df = load_dataset('penguins')\n",
    "\n",
    "# Cleaning the data\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b6e8b",
   "metadata": {},
   "source": [
    "Since ML algorithms cannot deal with categorical variables, we need to OneHotEncoder() them. As we remember, they are the variables 'Sex' and 'Island'. We will do it a little bit differently here, on the entire dataset instead of on the train and test arrays.\n",
    "\n",
    "Note: \n",
    "* The .categories_ attribute contains a list containing an array of the attribute names \n",
    "* The encoded object contains the one-hot encoded array. By converting it to an explicit array, the data can be mapped to DataFrame columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a09a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder()\n",
    "\n",
    "# For island\n",
    "encoded1 = one_hot.fit_transform(df[['island']])\n",
    "df[one_hot.categories_[0]] = encoded1.toarray()\n",
    "\n",
    "# For sex\n",
    "encoded2 = one_hot.fit_transform(df[['sex']])\n",
    "df[one_hot.categories_[0]] = encoded2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b4a6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>Biscoe</th>\n",
       "      <th>Dream</th>\n",
       "      <th>Torgersen</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>47.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4925.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0    Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1    Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2    Adelie  Torgersen            40.3           18.0              195.0   \n",
       "4    Adelie  Torgersen            36.7           19.3              193.0   \n",
       "5    Adelie  Torgersen            39.3           20.6              190.0   \n",
       "..      ...        ...             ...            ...                ...   \n",
       "338  Gentoo     Biscoe            47.2           13.7              214.0   \n",
       "340  Gentoo     Biscoe            46.8           14.3              215.0   \n",
       "341  Gentoo     Biscoe            50.4           15.7              222.0   \n",
       "342  Gentoo     Biscoe            45.2           14.8              212.0   \n",
       "343  Gentoo     Biscoe            49.9           16.1              213.0   \n",
       "\n",
       "     body_mass_g     sex  Biscoe  Dream  Torgersen  Female  Male  \n",
       "0         3750.0    Male     0.0    0.0        1.0     0.0   1.0  \n",
       "1         3800.0  Female     0.0    0.0        1.0     1.0   0.0  \n",
       "2         3250.0  Female     0.0    0.0        1.0     1.0   0.0  \n",
       "4         3450.0  Female     0.0    0.0        1.0     1.0   0.0  \n",
       "5         3650.0    Male     0.0    0.0        1.0     0.0   1.0  \n",
       "..           ...     ...     ...    ...        ...     ...   ...  \n",
       "338       4925.0  Female     1.0    0.0        0.0     1.0   0.0  \n",
       "340       4850.0  Female     1.0    0.0        0.0     1.0   0.0  \n",
       "341       5750.0    Male     1.0    0.0        0.0     0.0   1.0  \n",
       "342       5200.0  Female     1.0    0.0        0.0     1.0   0.0  \n",
       "343       5400.0    Male     1.0    0.0        0.0     0.0   1.0  \n",
       "\n",
       "[333 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "602e8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the original categorical variables from the dataframe\n",
    "df.drop(['sex', 'island'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c1edb",
   "metadata": {},
   "source": [
    "Documentation for the RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "The parameters are very similar to those for the Decision Tree.\n",
    "Note:\n",
    "- n_estimators is The number of trees in the forest. The default is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57ead3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our X and y\n",
    "X = df.copy()\n",
    "X.drop(['species'], axis = 1, inplace=True)\n",
    "\n",
    "y = df['species']\n",
    "\n",
    "# Splitting our data into training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f82639a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our classifier \n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1284b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "forest_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c9009eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing/Predicting using our model\n",
    "predictions = forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40917ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "# Checking its accuracy\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e70988",
   "metadata": {},
   "source": [
    "At home practice: apply random forest algorithm on titanic data and decision tree algorithm on penguins data. Compare and contrast the accuracy of the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
